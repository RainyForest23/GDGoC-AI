# GDGoC AI 2주차 회고 - 지도학습 vs 비지도학습

## I. Fact

### 실습 데이터셋
- **PIMA 당뇨병 데이터셋**: 768개 샘플, 9개 특성 (Glucose, BMI, Age 등)
- **MNIST 손글씨 데이터셋**: 1,797개 샘플, 64차원 (8×8 이미지)

### 구현한 모델과 성능
**지도학습 - 회귀 (BMI 예측)**
- Linear Regression: R² = 0.391
- Random Forest Regression: R² = 0.479

**지도학습 - 분류 (당뇨병 진단)**
- Logistic Regression: Accuracy = 87.66%
- Decision Tree: Accuracy = 90.26%

**비지도학습**
- PCA: 64차원 → 2차원 축소
- K-Means: 10개 클러스터 생성

### 주요 전처리 기법
- 결측치 처리: 제거 및 조건부 평균값 대체 (Insulin 48.7% 결측)
- 이상치 탐지: IQR 방법으로 24개 발견
- 스케일링: StandardScaler 적용

---

## II. Feeling
Introduction to Machine Learning (pf. 이형준) 강의를 수강하며 머신러닝 구현보다는 이론적/수학적인 부분에 대하여 배울 수 있었는데, 이번 세션을 통하여 실제로 실습을 진행하면서 어떻게 쓰이는지 알아갈 수 있게 되어 유익했다. 

그래서 Finding Part에는 실습에서 나온 내용들을 이론적 배경과 함께 정리함

---

## III. Finding

## 1. 지도학습 (Supervised Learning)

### 1.1 선형 회귀 (Linear Regression)

**이론적 배경**

선형 회귀는 입력 변수 X와 출력 변수 y 사이의 선형 관계를 모델링.

**수식:**
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```
- β₀: 절편 (intercept)
- βᵢ: i번째 특성의 가중치 (coefficient)
- ε: 오차항 (error term)

**학습 방법: 최소제곱법 (Ordinary Least Squares, OLS)**

목적 함수(손실 함수)를 최소화하는 β를 찾기

```
L(β) = Σ(yᵢ - ŷᵢ)² = Σ(yᵢ - (β₀ + β₁x₁ᵢ + ... + βₙxₙᵢ))²
```

이를 행렬 형태로 표현하면,
```
β = (XᵀX)⁻¹Xᵀy
```

**평가 지표**
- **MSE (Mean Squared Error)**: 평균 제곱 오차, 예측값과 실제값의 차이를 제곱하여 평균
- **R² Score (결정계수)**: 모델이 데이터 분산을 설명하는 비율 (0~1, 1에 가까울수록 좋음)
  ```
  R² = 1 - (RSS/TSS) = 1 - Σ(yᵢ-ŷᵢ)²/Σ(yᵢ-ȳ)²
  ```

**장점**: 해석 가능, 계산 빠름, 과적합 위험 낮음
**단점**: 선형 관계만 모델링, 이상치에 민감, 다중공선성 문제

---

### 1.2 로지스틱 회귀 (Logistic Regression)

**이론적 배경**

선형 결합 결과를 **시그모이드 함수**에 통과시켜 0~1 사이의 확률값으로 변환.

**Sigmoid 함수**
> *이 함수 수식이 왜 이렇게 익숙한가 했는데 평가원이 사랑하는... 함수라는 이야기가..*
```
σ(z) = 1 / (1 + e⁻ᶻ)
where z = β₀ + β₁x₁ + ... + βₙxₙ
```

**확률 해석:**
```
P(y=1|X) = σ(z)
P(y=0|X) = 1 - σ(z)
```

**학습 방법: 최대우도추정 (Maximum Likelihood Estimation, MLE)**

log-likelihood를 최대화
```
L(β) = Σ[yᵢ log(ŷᵢ) + (1-yᵢ)log(1-ŷᵢ)]
```


**규제 (Regularization)**
- **L1 (Lasso)**: `penalty = |β|` → 일부 계수를 0으로 만들어 특성 선택 효과
- **L2 (Ridge)**: `penalty = β²` → 계수를 작게 만들어 과적합 방지
- **C 파라미터**: 규제 강도의 역수 (C↓ = 강한 규제)

**장점**: 확률 출력, 해석 가능, 다중 클래스 확장 가능
**단점**: 선형 결정 경계, 특성 스케일링 필요

---

### 1.3 의사결정 트리 (Decision Tree)

**이론적 배경**

데이터를 **재귀적으로 분할**하여 트리 구조 만듦. 각 노드에서 특성과 임계값을 선택하여 데이터 분할.

**분할 기준 (Splitting Criterion)**

1. **지니 불순도 (Gini Impurity)**
   ```
   Gini(p) = 1 - Σpᵢ²
   ```
   - pᵢ: 클래스 i의 비율
   - 0에 가까울수록 순수 (한 클래스만 존재)

2. **엔트로피 (Entropy) & 정보 이득 (Information Gain)**
   ```
   Entropy(p) = -Σpᵢ log₂(pᵢ)
   IG = Entropy(parent) - Σ(nᵢ/n)·Entropy(childᵢ)
   ```

**분할 과정**
1. 모든 특성과 가능한 임계값 탐색
2. 불순도를 가장 많이 감소시키는 분할 선택
3. 재귀적으로 자식 노드 생성
4. 종료 조건: max_depth, min_samples_split 등

**과적합 방지**
- **사전 가지치기 (Pre-pruning)**: max_depth, min_samples_leaf 제한
- **사후 가지치기 (Post-pruning)**: ccp_alpha로 복잡도 기반 가지치기

**장점**: 해석 가능, 비선형 관계 학습, 특성 스케일링 불필요
**단점**: 과적합 경향, 불안정 (데이터 변화에 민감)

---

### 1.4 랜덤 포레스트 (Random Forest)

**이론적 배경**

앙상블(Ensemble) 학습의 일종으로, 여러 Decision Tree를 결합하여 예측.

**배깅 (Bagging, Bootstrap Aggregating)**
1. 원본 데이터에서 복원 추출로 n개의 부트스트랩 샘플 생성
2. 각 샘플로 독립적인 Decision Tree 학습
3. 회귀: 평균값, 분류: 다수결 투표로 최종 예측

**특성 무작위성 (Feature Randomness)**
- 각 노드 분할 시 전체 특성이 아닌 무작위로 선택된 일부 특성만 고려
- 트리 간 상관성 감소 → 일반화 성능 향상

**수식:**
```
ŷ = (1/B) Σ T_b(x)  (회귀)
ŷ = mode{T₁(x), T₂(x), ..., T_B(x)}  (분류)
```

**장점**: 과적합 방지, 높은 성능, Feature Importance 제공
**단점**: 해석 어려움, 학습 시간/메모리 사용 많음

---

## 2. 비지도학습 (Unsupervised Learning)
> *이건 일단 잘 모르겠다.. 공부를 안한..*

### 2.1 주성분 분석 (PCA, Principal Component Analysis)

**수학적 정의**

데이터 행렬 X (n×d)에 대해:
1. 데이터 중심화: X̄ = X - mean(X)
2. 공분산 행렬 계산: C = (1/n)X̄ᵀX̄
3. 고유값 분해: C·v = λ·v
4. 고유값이 큰 순서대로 고유벡터 선택

**주성분 (Principal Component)**
- 1차 주성분 (PC1): 데이터 분산이 최대인 방향
- 2차 주성분 (PC2): PC1과 직교하며 분산이 두 번째로 큰 방향
- k차 주성분: 이전 성분들과 모두 직교

**차원 축소:**
```
Z = X · W
```
- W: 상위 k개 고유벡터로 구성된 변환 행렬 (d×k)
- Z: 축소된 데이터 (n×k)

**분산 보존율:**
```
설명된 분산 비율 = Σλₖ / Σλᵢ
```

**활용**
- 시각화 (2D/3D)
- 차원의 저주 완화
- 노이즈 제거
- 전처리 (다중공선성 해소)

**장점**: 수학적으로 명확, 계산 효율적
**단점**: 선형 변환만 가능, 해석 어려움 (새로운 축의 의미)

---

### 2.2 K-평균 군집화 (K-Means Clustering)

**이론적 배경**

데이터를 k개의 클러스터로 분할하되, **각 클러스터 내 분산(inertia)을 최소화**.

**목적 함수:**
```
J = Σ Σ ||xᵢ - μⱼ||²
    j=1 i∈Cⱼ
```
- μⱼ: 클러스터 j의 중심 (centroid)
- Cⱼ: 클러스터 j에 속한 데이터 집합

**알고리즘 (Lloyd's Algorithm)**
1. **초기화**: k개의 중심점 무작위 선택
2. **할당 단계 (Assignment)**: 각 데이터를 가장 가까운 중심에 할당
   ```
   cᵢ = argmin ||xᵢ - μⱼ||²
          j
   ```
3. **갱신 단계 (Update)**: 각 클러스터의 중심 재계산
   ```
   μⱼ = (1/|Cⱼ|) Σ xᵢ
                i∈Cⱼ
   ```
4. 수렴할 때까지 2-3 반복 (중심이 변하지 않거나 변화가 임계값 이하)

**초기화 문제**
- 무작위 초기화는 local minimum에 빠질 수 있음
- **K-Means++**: 중심들이 서로 멀리 떨어지도록 초기화 (scikit-learn 기본값)

**최적 k 선택 방법**
1. **Elbow Method**: inertia 그래프에서 급격히 감소가 둔화되는 지점
2. **Silhouette Score**: 클러스터 내 응집도와 클러스터 간 분리도 측정
   ```
   s(i) = (b(i) - a(i)) / max{a(i), b(i)}
   ```

**장점**: 단순하고 빠름, 대용량 데이터 처리 가능
**단점**: k 사전 지정 필요, 구형 클러스터 가정, 이상치에 민감

---

## 3. 데이터 전처리 이론

### 3.1 결측치 처리

**삭제 방법 (Deletion)**
- **Listwise**: 결측치 있는 행 전체 제거 → 데이터 손실 크지만 편향 없음
- **Pairwise**: 분석마다 가용 데이터만 사용

**대체 방법 (Imputation)**
- **평균/중앙값/최빈값**: 단순하지만 분산 감소
- **조건부 대체**: 그룹별 통계량 (예: Outcome별 Insulin 평균)
- **회귀 대체**: 다른 변수로 결측치 예측
- **KNN Imputation**: k개 이웃의 평균값
- **MICE (다중 대체)**: 여러 번 대체하여 불확실성 반영

**선택 기준**
- 결측 비율 < 5%: 삭제
- 5~20%: 대체 (평균/중앙값)
- > 20%: 도메인 지식 활용 또는 결측 여부를 새로운 특성으로

---

### 3.2 이상치 탐지

**IQR 방법**
```
Q1 = 25th percentile
Q3 = 75th percentile
IQR = Q3 - Q1
Lower bound = Q1 - 1.5·IQR
Upper bound = Q3 + 1.5·IQR
```

**Z-score 방법**
```
z = (x - μ) / σ
|z| > 3 이면 이상치
```

**처리 방법**
- 제거: 명백한 오류일 때
- 변환: 로그/제곱근 변환으로 영향 감소
- 캡핑: 상하한값으로 대체
- 분리 분석: 이상치 그룹 별도 분석

---

### 3.3 특성 스케일링

**표준화 (Standardization)**
```
x' = (x - μ) / σ
```
- 평균 0, 표준편차 1로 변환
- 이상치 영향 받음
- Linear/Logistic Regression, SVM, PCA에 적합

**정규화 (Normalization, Min-Max Scaling)**
```
x' = (x - min) / (max - min)
```
- 0~1 범위로 변환
- 이상치에 매우 민감
- Neural Network, K-Means에 적합

**필요한 모델**: 거리 기반 (KNN, K-Means), 경사하강법 사용 (Linear, Logistic)
**불필요한 모델**: 트리 기반 (Decision Tree, Random Forest, XGBoost)

---

## 4. 학습 결과 및 통찰

### 4.1 모델 선택 전략

**베이스라인 모델**
- 회귀: Linear Regression
- 분류: Logistic Regression
- 이유: 빠르고 해석 가능, 성능 기준점 제공

**성능 개선 단계**
1. 데이터 품질 향상 (전처리 강화)
2. 복잡한 모델 시도 (Tree, Forest)
3. 하이퍼파라미터 튜닝
4. 앙상블 기법
